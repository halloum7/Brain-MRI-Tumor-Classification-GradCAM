# -*- coding: utf-8 -*-
"""A Stratified 5-Fold.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ndVZBcCg-xdCfK07WTfUDyClIMk2e0e6
"""

# !nvidia-smi

# Upload kaggle.json
from google.colab import files
files.upload()

!pip install kaggle

import os

os.makedirs("/root/.kaggle", exist_ok=True)
!mv kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d sartajbhuvaji/brain-tumor-classification-mri

!unzip brain-tumor-classification-mri.zip

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tqdm import tqdm

train_path = "/content/Training"
test_path = "/content/Testing"

IMG_SIZE = 224
def load_data(folder_path):
    data = []
    labels = []

    for category in os.listdir(folder_path):
        category_path = os.path.join(folder_path, category)

        for img in tqdm(os.listdir(category_path)):
            img_path = os.path.join(category_path, img)

            image = cv2.imread(img_path)
            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))
            image = image / 255.0

            # üî• CORRECTION ICI
            category_clean = category.strip().lower()

            if category_clean == "no_tumor":
                label = 0
            else:
                label = 1

            data.append(image)
            labels.append(label)

    return np.array(data), np.array(labels)

X_train_full, y_train_full = load_data(train_path)
X_test_full, y_test_full = load_data(test_path)

X = np.concatenate((X_train_full, X_test_full), axis=0)
y = np.concatenate((y_train_full, y_test_full), axis=0)

X = X.astype(np.float32)

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)
print("Class distribution in train:", np.bincount(y_train))
print("Class distribution in test:", np.bincount(y_test))

from skimage.feature import hog
from sklearn.preprocessing import StandardScaler

def extract_hog_features(X):
    hog_features = []

    for image in tqdm(X):
        gray = cv2.cvtColor((image*255).astype("uint8"), cv2.COLOR_BGR2GRAY)

        features = hog(
            gray,
            orientations=9,
            pixels_per_cell=(8,8),
            cells_per_block=(2,2),
            block_norm='L2-Hys'
        )

        hog_features.append(features)

    return np.array(hog_features)

X_train_hog = extract_hog_features(X_train)
X_test_hog = extract_hog_features(X_test)

scaler = StandardScaler()

X_train_hog = scaler.fit_transform(X_train_hog)
X_test_hog = scaler.transform(X_test_hog)

# from sklearn.decomposition import PCA

# # R√©duction de dimension
# pca = PCA(n_components=200)   # vous pouvez tester 100, 150 ou 200

# X_train_hog = pca.fit_transform(X_train_hog)
# X_test_hog = pca.transform(X_test_hog)

# print("Nouvelle dimension apr√®s PCA :", X_train_hog.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score
import xgboost as xgb

print("Classes d√©tect√©es :", os.listdir(train_path))

print("Training folders:", os.listdir(train_path))
print("Testing folders:", os.listdir(test_path))

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_hog, y_train)

lr_pred = lr_model.predict(X_test_hog)

print("Logistic Regression Accuracy:",
      accuracy_score(y_test, lr_pred))

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_hog, y_train)

rf_pred = rf_model.predict(X_test_hog)

print("Random Forest Accuracy:",
      accuracy_score(y_test, rf_pred))

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    roc_curve
)
import matplotlib.pyplot as plt

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_hog, y_train)

lr_pred = lr_model.predict(X_test_hog)
lr_proba = lr_model.predict_proba(X_test_hog)[:, 1]

print("----- Logistic Regression -----")
print("Accuracy:", accuracy_score(y_test, lr_pred))
print("Precision:", precision_score(y_test, lr_pred))
print("Recall:", recall_score(y_test, lr_pred))
print("F1-score:", f1_score(y_test, lr_pred))
print("AUC:", roc_auc_score(y_test, lr_proba))

print("Confusion Matrix:\n", confusion_matrix(y_test, lr_pred))

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_hog, y_train)

rf_pred = rf_model.predict(X_test_hog)
rf_proba = rf_model.predict_proba(X_test_hog)[:, 1]

print("----- Random Forest -----")
print("Accuracy:", accuracy_score(y_test, rf_pred))
print("Precision:", precision_score(y_test, rf_pred))
print("Recall:", recall_score(y_test, rf_pred))
print("F1-score:", f1_score(y_test, rf_pred))
print("AUC:", roc_auc_score(y_test, rf_proba))

print("Confusion Matrix:\n", confusion_matrix(y_test, rf_pred))

xgb_model = xgb.XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    n_estimators=100,
    booster='gbtree',  # 'gblinear' peut √™tre plus rapide si dataset pas trop grand
    n_jobs=-1          # utilise tous les CPU disponibles
)
xgb_model.fit(X_train_hog, y_train)

xgb_pred = xgb_model.predict(X_test_hog)
xgb_proba = xgb_model.predict_proba(X_test_hog)[:, 1]

print("----- XGBoost -----")
print("Accuracy:", accuracy_score(y_test, xgb_pred))
print("Precision:", precision_score(y_test, xgb_pred))
print("Recall:", recall_score(y_test, xgb_pred))
print("F1-score:", f1_score(y_test, xgb_pred))
print("AUC:", roc_auc_score(y_test, xgb_proba))

print("Confusion Matrix:\n", confusion_matrix(y_test, xgb_pred))

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

# === Fonction pour afficher et sauvegarder ===
def plot_and_save_confusion_matrix(y_true, y_pred, model_name, filename):
    cm = confusion_matrix(y_true, y_pred)

    plt.figure()
    plt.imshow(cm)
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.xticks([0,1])
    plt.yticks([0,1])

    for i in range(2):
        for j in range(2):
            plt.text(j, i, cm[i, j],
                     ha="center", va="center")

    plt.colorbar()
    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    plt.show()


# === Logistic Regression ===
plot_and_save_confusion_matrix(
    y_test, lr_pred,
    "Logistic Regression",
    "confusion_matrix_logistic.png"
)

# === Random Forest ===
plot_and_save_confusion_matrix(
    y_test, rf_pred,
    "Random Forest",
    "confusion_matrix_rf.png"
)

# === XGBoost ===
plot_and_save_confusion_matrix(
    y_test, xgb_pred,
    "XGBoost",
    "confusion_matrix_xgb.png"
)

from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    balanced_accuracy_score,
    matthews_corrcoef,
    cohen_kappa_score
)
import numpy as np
import matplotlib.pyplot as plt

# === 1. Charger VGG16 pr√©-entra√Æn√© ===
base_model_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))

# Fine-tuning partiel : on g√®le toutes sauf les 4 derni√®res couches convolutionnelles
for layer in base_model_vgg.layers[:-4]:
    layer.trainable = False
for layer in base_model_vgg.layers[-4:]:
    layer.trainable = True

# Ajouter couche classification
x = base_model_vgg.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
output = Dense(1, activation='sigmoid')(x)

model_vgg = Model(inputs=base_model_vgg.input, outputs=output)

model_vgg.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model_vgg.summary()

# === 2. Preprocessing sp√©cifique VGG16 ===
X_train_vgg = preprocess_input(X_train * 255)
X_test_vgg = preprocess_input(X_test * 255)

# === 3. Gestion du d√©s√©quilibre ===
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights))

# === 4. Entra√Ænement VGG16 ===
history_vgg = model_vgg.fit(
    X_train_vgg,
    y_train,
    validation_data=(X_test_vgg, y_test),
    epochs=15,
    batch_size=32,
    class_weight=class_weights
)

# === 5. Pr√©dictions & m√©triques ===
y_pred_proba_vgg = model_vgg.predict(X_test_vgg)
y_pred_vgg = (y_pred_proba_vgg > 0.5).astype(int)

accuracy_vgg = accuracy_score(y_test, y_pred_vgg)
precision_vgg = precision_score(y_test, y_pred_vgg)
recall_vgg = recall_score(y_test, y_pred_vgg)
f1_vgg = f1_score(y_test, y_pred_vgg)
auc_vgg = roc_auc_score(y_test, y_pred_proba_vgg)
balanced_acc = balanced_accuracy_score(y_test, y_pred_vgg)
mcc = matthews_corrcoef(y_test, y_pred_vgg)
kappa = cohen_kappa_score(y_test, y_pred_vgg)

print("----- VGG16 (corrig√©) -----")
print("Accuracy:", accuracy_vgg)
print("Precision:", precision_vgg)
print("Recall:", recall_vgg)
print("F1-score:", f1_vgg)
print("AUC:", auc_vgg)
print("Balanced Accuracy:", balanced_acc)
print("MCC:", mcc)
print("Cohen Kappa:", kappa)

# === 6. Matrice de confusion VGG16 ===
cm_vgg = confusion_matrix(y_test, y_pred_vgg)

plt.figure()
plt.imshow(cm_vgg)
plt.title("Confusion Matrix - VGG16 (corrig√©)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.xticks([0,1])
plt.yticks([0,1])

for i in range(2):
    for j in range(2):
        plt.text(j, i, cm_vgg[i,j], ha="center", va="center")

plt.colorbar()
plt.tight_layout()
plt.savefig("confusion_matrix_vgg16_corrected.png", dpi=300)
plt.show()

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    balanced_accuracy_score,
    matthews_corrcoef,
    cohen_kappa_score
)
import numpy as np
import matplotlib.pyplot as plt

# === 1. Charger ResNet50 pr√©-entra√Æn√© ===
base_model_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))

# Fine-tuning partiel : on g√®le toutes sauf les 4 derni√®res couches convolutionnelles
for layer in base_model_resnet.layers[:-4]:
    layer.trainable = False
for layer in base_model_resnet.layers[-4:]:
    layer.trainable = True

# Ajouter couche classification
x = base_model_resnet.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
output = Dense(1, activation='sigmoid')(x)

model_resnet = Model(inputs=base_model_resnet.input, outputs=output)

model_resnet.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model_resnet.summary()

# === 2. Preprocessing officiel ResNet50 ===
X_train_resnet = preprocess_input(X_train * 255)
X_test_resnet = preprocess_input(X_test * 255)

# === 3. Gestion du d√©s√©quilibre ===
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights))

# === 4. Entra√Ænement ResNet50 ===
history_resnet = model_resnet.fit(
    X_train_resnet,
    y_train,
    validation_data=(X_test_resnet, y_test),
    epochs=15,
    batch_size=32,
    class_weight=class_weights
)

# === 5. Pr√©dictions & m√©triques ===
y_pred_proba_resnet = model_resnet.predict(X_test_resnet)
y_pred_resnet = (y_pred_proba_resnet > 0.5).astype(int)

accuracy_resnet = accuracy_score(y_test, y_pred_resnet)
precision_resnet = precision_score(y_test, y_pred_resnet)
recall_resnet = recall_score(y_test, y_pred_resnet)
f1_resnet = f1_score(y_test, y_pred_resnet)
auc_resnet = roc_auc_score(y_test, y_pred_proba_resnet)
balanced_acc_resnet = balanced_accuracy_score(y_test, y_pred_resnet)
mcc_resnet = matthews_corrcoef(y_test, y_pred_resnet)
kappa_resnet = cohen_kappa_score(y_test, y_pred_resnet)

print("----- ResNet50 (corrig√©) -----")
print("Accuracy:", accuracy_resnet)
print("Precision:", precision_resnet)
print("Recall:", recall_resnet)
print("F1-score:", f1_resnet)
print("AUC:", auc_resnet)
print("Balanced Accuracy:", balanced_acc_resnet)
print("MCC:", mcc_resnet)
print("Cohen Kappa:", kappa_resnet)

# === 6. Matrice de confusion ResNet50 ===
cm_resnet = confusion_matrix(y_test, y_pred_resnet)

plt.figure()
plt.imshow(cm_resnet)
plt.title("Confusion Matrix - ResNet50 (corrig√©)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.xticks([0,1])
plt.yticks([0,1])

for i in range(2):
    for j in range(2):
        plt.text(j, i, cm_resnet[i,j], ha="center", va="center")

plt.colorbar()
plt.tight_layout()
plt.savefig("confusion_matrix_resnet50_corrected.png", dpi=300)
plt.show()

import tensorflow as tf
import cv2
import numpy as np
import matplotlib.pyplot as plt

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    # 1Ô∏è‚É£ Cr√©er le mod√®le Grad-CAM
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.output]
    )

    # 2Ô∏è‚É£ Calcul du gradient
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# === Exemple sur une image test ===
img = X_test[0]  # premi√®re image du test
img_array = np.expand_dims(img, axis=0)

# --- Grad-CAM ResNet50 ---
heatmap_resnet = make_gradcam_heatmap(img_array, model_resnet, "conv5_block3_out")
heatmap_resnet = cv2.resize(heatmap_resnet, (224,224))
heatmap_resnet = np.uint8(255 * heatmap_resnet)
heatmap_resnet = cv2.applyColorMap(heatmap_resnet, cv2.COLORMAP_JET)
superimposed_resnet = heatmap_resnet * 0.4 + (img*255).astype(np.uint8)

# --- Grad-CAM VGG16 ---
heatmap_vgg = make_gradcam_heatmap(img_array, model_vgg, "block5_conv3")
heatmap_vgg = cv2.resize(heatmap_vgg, (224,224))
heatmap_vgg = np.uint8(255 * heatmap_vgg)
heatmap_vgg = cv2.applyColorMap(heatmap_vgg, cv2.COLORMAP_JET)
superimposed_vgg = heatmap_vgg * 0.4 + (img*255).astype(np.uint8)

# --- Affichage c√¥te √† c√¥te ---
plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.imshow(superimposed_resnet)
plt.title("Grad-CAM - ResNet50")
plt.axis('off')

plt.subplot(1,2,2)
plt.imshow(superimposed_vgg)
plt.title("Grad-CAM - VGG16")
plt.axis('off')

plt.show()

# ============================================
# STRATIFIED 5-FOLD CROSS VALIDATION
# ============================================

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, balanced_accuracy_score,
    matthews_corrcoef, cohen_kappa_score
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

from tensorflow.keras.applications import VGG16, ResNet50
from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg
from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam

import numpy as np

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

results_lr = []
results_rf = []
results_xgb = []
results_vgg = []
results_resnet = []

# ============================================
# LOOP OVER FOLDS
# ============================================

for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):

    print(f"\n========== FOLD {fold+1} ==========")

    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # ======================================
    # ML MODELS (HOG)
    # ======================================

    X_train_hog = extract_hog_features(X_train)
    X_test_hog = extract_hog_features(X_test)

    scaler = StandardScaler()
    X_train_hog = scaler.fit_transform(X_train_hog)
    X_test_hog = scaler.transform(X_test_hog)

    # ---- Logistic Regression ----
    lr = LogisticRegression(max_iter=1000)
    lr.fit(X_train_hog, y_train)
    y_pred = lr.predict(X_test_hog)
    y_proba = lr.predict_proba(X_test_hog)[:,1]

    results_lr.append([
        accuracy_score(y_test, y_pred),
        precision_score(y_test, y_pred),
        recall_score(y_test, y_pred),
        f1_score(y_test, y_pred),
        roc_auc_score(y_test, y_proba),
        balanced_accuracy_score(y_test, y_pred),
        matthews_corrcoef(y_test, y_pred),
        cohen_kappa_score(y_test, y_pred)
    ])

    # ---- Random Forest ----
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train_hog, y_train)
    y_pred = rf.predict(X_test_hog)
    y_proba = rf.predict_proba(X_test_hog)[:,1]

    results_rf.append([
        accuracy_score(y_test, y_pred),
        precision_score(y_test, y_pred),
        recall_score(y_test, y_pred),
        f1_score(y_test, y_pred),
        roc_auc_score(y_test, y_proba),
        balanced_accuracy_score(y_test, y_pred),
        matthews_corrcoef(y_test, y_pred),
        cohen_kappa_score(y_test, y_pred)
    ])

    # ---- XGBoost ----
    xgb_model = xgb.XGBClassifier(
        use_label_encoder=False,
        eval_metric='logloss',
        n_estimators=100
    )
    xgb_model.fit(X_train_hog, y_train)
    y_pred = xgb_model.predict(X_test_hog)
    y_proba = xgb_model.predict_proba(X_test_hog)[:,1]

    results_xgb.append([
        accuracy_score(y_test, y_pred),
        precision_score(y_test, y_pred),
        recall_score(y_test, y_pred),
        f1_score(y_test, y_pred),
        roc_auc_score(y_test, y_proba),
        balanced_accuracy_score(y_test, y_pred),
        matthews_corrcoef(y_test, y_pred),
        cohen_kappa_score(y_test, y_pred)
    ])

    # ======================================
    # VGG16
    # ======================================

    X_train_vgg = preprocess_vgg(X_train * 255)
    X_test_vgg = preprocess_vgg(X_test * 255)

    base_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))

    for layer in base_vgg.layers[:-4]:
        layer.trainable = False

    x = base_vgg.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.5)(x)
    output = Dense(1, activation='sigmoid')(x)

    model_vgg = Model(inputs=base_vgg.input, outputs=output)

    model_vgg.compile(
        optimizer=Adam(1e-4),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    model_vgg.fit(X_train_vgg, y_train, epochs=10, batch_size=32, verbose=0)

    y_proba = model_vgg.predict(X_test_vgg)
    y_pred = (y_proba > 0.5).astype(int)

    results_vgg.append([
        accuracy_score(y_test, y_pred),
        precision_score(y_test, y_pred),
        recall_score(y_test, y_pred),
        f1_score(y_test, y_pred),
        roc_auc_score(y_test, y_proba),
        balanced_accuracy_score(y_test, y_pred),
        matthews_corrcoef(y_test, y_pred),
        cohen_kappa_score(y_test, y_pred)
    ])

    # ======================================
    # ResNet50
    # ======================================

    X_train_res = preprocess_resnet(X_train * 255)
    X_test_res = preprocess_resnet(X_test * 255)

    base_res = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))

    for layer in base_res.layers[:-10]:
        layer.trainable = False

    x = base_res.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.5)(x)
    output = Dense(1, activation='sigmoid')(x)

    model_res = Model(inputs=base_res.input, outputs=output)

    model_res.compile(
        optimizer=Adam(1e-4),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    model_res.fit(X_train_res, y_train, epochs=10, batch_size=32, verbose=0)

    y_proba = model_res.predict(X_test_res)
    y_pred = (y_proba > 0.5).astype(int)

    results_resnet.append([
        accuracy_score(y_test, y_pred),
        precision_score(y_test, y_pred),
        recall_score(y_test, y_pred),
        f1_score(y_test, y_pred),
        roc_auc_score(y_test, y_proba),
        balanced_accuracy_score(y_test, y_pred),
        matthews_corrcoef(y_test, y_pred),
        cohen_kappa_score(y_test, y_pred)
    ])

# ============================================
# SUMMARY FUNCTION
# ============================================

def summarize(results, name):
    results = np.array(results)
    metrics = ["Accuracy","Precision","Recall","F1","AUC","Balanced Acc","MCC","Kappa"]

    print(f"\n===== {name} - 5 Fold Results =====")
    for i, metric in enumerate(metrics):
        print(f"{metric}: {results[:,i].mean():.4f} ¬± {results[:,i].std():.4f}")

summarize(results_lr, "Logistic Regression")
summarize(results_rf, "Random Forest")
summarize(results_xgb, "XGBoost")
summarize(results_vgg, "VGG16")
summarize(results_resnet, "ResNet50")

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_curve, auc
import numpy as np

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# === Stockage global ===
y_true_all = []

lr_proba_all = []
rf_proba_all = []
xgb_proba_all = []
vgg_proba_all = []
resnet_proba_all = []

lr_pred_all = []
rf_pred_all = []
xgb_pred_all = []
vgg_pred_all = []
resnet_pred_all = []

for train_idx, val_idx in skf.split(X, y):

    X_train_fold, X_val_fold = X[train_idx], X[val_idx]
    y_train_fold, y_val_fold = y[train_idx], y[val_idx]

    # ===============================
    # HOG FEATURES (Classical Models)
    # ===============================
    X_train_hog = extract_hog_features(X_train_fold)
    X_val_hog = extract_hog_features(X_val_fold)

    scaler = StandardScaler()
    X_train_hog = scaler.fit_transform(X_train_hog)
    X_val_hog = scaler.transform(X_val_hog)

    # Logistic Regression
    lr_model = LogisticRegression(max_iter=1000)
    lr_model.fit(X_train_hog, y_train_fold)
    lr_proba = lr_model.predict_proba(X_val_hog)[:,1]
    lr_pred = (lr_proba > 0.5).astype(int)

    # Random Forest
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_model.fit(X_train_hog, y_train_fold)
    rf_proba = rf_model.predict_proba(X_val_hog)[:,1]
    rf_pred = (rf_proba > 0.5).astype(int)

    # XGBoost
    xgb_model = xgb.XGBClassifier(
        use_label_encoder=False,
        eval_metric='logloss',
        n_estimators=100,
        n_jobs=-1
    )
    xgb_model.fit(X_train_hog, y_train_fold)
    xgb_proba = xgb_model.predict_proba(X_val_hog)[:,1]
    xgb_pred = (xgb_proba > 0.5).astype(int)

    # ===============================
    # Deep Learning Models
    # ===============================
    X_train_dl = preprocess_input(X_train_fold * 255)
    X_val_dl = preprocess_input(X_val_fold * 255)

    # VGG16
    model_vgg.fit(X_train_dl, y_train_fold,
                  epochs=5,
                  batch_size=32,
                  verbose=0)

    vgg_proba = model_vgg.predict(X_val_dl).flatten()
    vgg_pred = (vgg_proba > 0.5).astype(int)

    # ResNet50
    model_resnet.fit(X_train_dl, y_train_fold,
                     epochs=5,
                     batch_size=32,
                     verbose=0)

    resnet_proba = model_resnet.predict(X_val_dl).flatten()
    resnet_pred = (resnet_proba > 0.5).astype(int)

    # ===============================
    # Stockage global
    # ===============================
    y_true_all.extend(y_val_fold)

    lr_proba_all.extend(lr_proba)
    rf_proba_all.extend(rf_proba)
    xgb_proba_all.extend(xgb_proba)
    vgg_proba_all.extend(vgg_proba)
    resnet_proba_all.extend(resnet_proba)

    lr_pred_all.extend(lr_pred)
    rf_pred_all.extend(rf_pred)
    xgb_pred_all.extend(xgb_pred)
    vgg_pred_all.extend(vgg_pred)
    resnet_pred_all.extend(resnet_pred)

print("Global predictions stored successfully.")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
import numpy as np

# Calcul ROC pour chaque mod√®le
fpr_lr, tpr_lr, _ = roc_curve(y_true_all, lr_proba_all)
fpr_rf, tpr_rf, _ = roc_curve(y_true_all, rf_proba_all)
fpr_xgb, tpr_xgb, _ = roc_curve(y_true_all, xgb_proba_all)
fpr_vgg, tpr_vgg, _ = roc_curve(y_true_all, vgg_proba_all)
fpr_res, tpr_res, _ = roc_curve(y_true_all, resnet_proba_all)

auc_lr = auc(fpr_lr, tpr_lr)
auc_rf = auc(fpr_rf, tpr_rf)
auc_xgb = auc(fpr_xgb, tpr_xgb)
auc_vgg = auc(fpr_vgg, tpr_vgg)
auc_res = auc(fpr_res, tpr_res)

plt.figure()

plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC={auc_lr:.3f})')
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC={auc_rf:.3f})')
plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC={auc_xgb:.3f})')
plt.plot(fpr_vgg, tpr_vgg, label=f'VGG16 (AUC={auc_vgg:.3f})')
plt.plot(fpr_res, tpr_res, label=f'ResNet50 (AUC={auc_res:.3f})')

plt.plot([0,1],[0,1],'k--')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison - 5 Fold Cross Validation')
plt.legend(loc='lower right')
plt.tight_layout()

plt.savefig("Figure1_ROC_Comparison.png", dpi=300)
plt.show()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

cm = confusion_matrix(y_true_all, resnet_pred_all)

plt.figure()
plt.imshow(cm)
plt.title("Confusion Matrix - ResNet50 (5-Fold CV)")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.xticks([0,1])
plt.yticks([0,1])

for i in range(2):
    for j in range(2):
        plt.text(j, i, cm[i, j],
                 ha="center", va="center")

plt.colorbar()
plt.tight_layout()

plt.savefig("Figure2_ConfusionMatrix_ResNet50.png", dpi=300)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

models = ["LogReg", "RF", "XGB", "VGG16", "ResNet50"]

f1_means = [0.9815, 0.9724, 0.9734, 0.9892, 0.9930]
f1_std = [0.0050, 0.0031, 0.0017, 0.0040, 0.0025]

plt.figure()

plt.bar(models, f1_means, yerr=f1_std)

plt.ylabel("F1-score")
plt.title("F1-score Comparison (5-Fold CV)")
plt.ylim(0.94, 1.00)

plt.tight_layout()
plt.savefig("Figure3_F1_Comparison.png", dpi=300)
plt.show()

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import cv2

# ==========================================
# 1Ô∏è‚É£ Fonction Grad-CAM
# ==========================================

def make_gradcam_heatmap(img_array, model, last_conv_layer_name):

    grad_model = tf.keras.models.Model(
        inputs=model.inputs,
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        loss = predictions[:, 0]

    grads = tape.gradient(loss, conv_outputs)

    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    heatmap = tf.maximum(heatmap, 0)
    heatmap /= tf.reduce_max(heatmap)

    return heatmap.numpy()


# ==========================================
# 2Ô∏è‚É£ S√©lectionner une image TEST
# ==========================================

# Choisir une image tumor positive (recommand√©)
for i in range(len(X)):
    if y[i] == 1:   # 1 = tumor
        img = X[i]
        true_label = y[i]
        break

img_array = np.expand_dims(img, axis=0)

# Important : preprocessing ResNet
from tensorflow.keras.applications.resnet50 import preprocess_input
img_array = preprocess_input(img_array * 255)


# ==========================================
# 3Ô∏è‚É£ G√©n√©rer la heatmap
# ==========================================

last_conv_layer_name = "conv5_block3_out"  # couche finale ResNet50

heatmap = make_gradcam_heatmap(img_array, model_res, last_conv_layer_name)


# ==========================================
# 4Ô∏è‚É£ Superposition Heatmap + Image
# ==========================================

# Image originale pour affichage
img_original = (img * 255).astype("uint8")

heatmap_resized = cv2.resize(heatmap, (224,224))
heatmap_colored = cv2.applyColorMap(
    np.uint8(255 * heatmap_resized),
    cv2.COLORMAP_JET
)

overlay = cv2.addWeighted(
    img_original,
    0.6,
    heatmap_colored,
    0.4,
    0
)


# ==========================================
# 5Ô∏è‚É£ Figure Publication-Ready
# ==========================================

plt.figure(figsize=(12,4))

plt.subplot(1,3,1)
plt.imshow(img_original)
plt.title("Original MRI")
plt.axis("off")

plt.subplot(1,3,2)
plt.imshow(heatmap_resized, cmap="jet")
plt.title("Grad-CAM Heatmap")
plt.axis("off")

plt.subplot(1,3,3)
plt.imshow(overlay)
plt.title("Overlay Visualization")
plt.axis("off")

plt.tight_layout()
plt.savefig("Figure_GradCAM_ResNet50.png", dpi=300)
plt.show()

print("Grad-CAM Figure saved successfully.")